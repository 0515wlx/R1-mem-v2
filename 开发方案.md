# Titans 模型开发方案

## 1. 概述  
Titans 是一种新型的深度学习架构，旨在解决长序列建模中的记忆问题。其核心是通过引入长期记忆模块（Neural Long-Term Memory），结合短时记忆（注意力机制）和持久记忆（任务相关知识），实现对历史信息的高效存储和检索。  

## 2. 架构设计  
Titans 包含三个核心模块：  
1. **核心模块**：负责处理当前上下文，使用注意力机制实现短时记忆。  
2. **长期记忆模块**：通过神经网络参数存储历史信息，支持在线学习和遗忘机制。  
3. **持久记忆模块**：存储与任务相关的知识，独立于输入数据。  

## 3. 长期记忆模块设计  

### 3.1 学习过程与惊喜度量  
长期记忆模块通过在线学习机制存储历史信息。其核心思想是通过惊喜度量（Surprise Metric）决定哪些信息需要存储。  
惊喜度量定义为模型对输入的梯度，梯度越大表示输入越具“惊喜性”。  
更新公式为：  
\[
M_t = M_{t-1} + S_t
\]
\[
S_t = \eta_t S_{t-1} - \theta_t \nabla \mathcal{L}(M_{t-1}; x_t)
\]  
其中，\(\eta_t\) 控制惊喜的衰减，\(\theta_t\) 控制当前惊喜的影响。  

### 3.2 遗忘机制  
为管理记忆容量，引入遗忘机制：  
\[
M_t = (1 - \alpha_t) M_{t-1} + S_t
\]  
\(\alpha_t\) 控制遗忘程度，范围为 [0, 1]。  

### 3.3 记忆检索  
通过前向传播（不更新权重）从记忆中检索信息：  
\[
y_t = M^*(q_t)
\]  
其中，\(q_t = x_t W_q\) 是查询的投影。  

## 4. Titans 变体设计  

### 4.1 MAC（Memory as a Context）  
将记忆作为上下文的扩展。具体步骤：  
1. 将输入分段，每段作为当前上下文，其前一段作为历史信息。  
2. 使用查询 \(q_t = s^{(l)} W_q\) 从记忆中检索对应信息 \(h_t = M_{t-1}(q_t)\)。  
3. 将检索到的信息与持久记忆和当前上下文拼接，输入注意力模块：  
\[
\bar{s}^{(l)} = [p_1 \; p_2 \; \cdots \; p_N \; || \; h_t \; || \; s^{(l)}]
\]  
4. 更新记忆并输出结果。  

### 4.2 MAG（Memory as a Gate）  
通过门控机制结合记忆与核心模块。具体步骤：  
1. 将持久记忆与输入拼接：  
\[
\hat{x} = [p_1, p_2, \ldots, p_N] \| x
\]  
2. 使用滑动窗口注意力处理拼接后的输入：  
\[
y = SW\text{-}Attn^*(\hat{x})
\]  
3. 通过门控机制结合记忆输出：  
\[
o = g(M(\hat{x}))
\]  

### 4.3 MAL（Memory as a Layer）  
将记忆作为网络的一层，与注意力模块堆叠。具体步骤：  
1. 将持久记忆与输入拼接：  
\[
\hat{x} = [p_1, p_2, \ldots, p_N] \| x
\]  
2. 通过记忆层压缩历史和当前上下文：  
\[
y = M(\hat{x})
\]  
3. 使用滑动窗口注意力处理压缩后的信息：  
\[
o = SW\text{-}Attn(y)
\]  

## 5. 实验与评估  
Titans 在多种任务上进行了评估，包括语言建模、常识推理、时间序列预测和基因组建模。实验结果表明，Titans 在长上下文任务中优于 Transformer 和现代线性循环模型。  

## 6. 结论  
Titans 通过引入长期记忆模块，解决了长序列建模中的记忆管理和检索问题。其三种变体（MAC、MAG、MAL）在不同任务中展示了各自的优势，为长上下文建模提供了新的解决方案。